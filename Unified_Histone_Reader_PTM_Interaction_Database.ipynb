{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hBsV1-B3Llu1",
        "outputId": "fcd704a9-cc23-4d52-ea28-5705372a827b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Upload your latest kd_master .xlsx:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ae3acae3-a00e-46e3-b232-71cbca7b4f22\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ae3acae3-a00e-46e3-b232-71cbca7b4f22\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kd_master_new_updated (2) (1) (7) (1) (2) (1) (5) (1) (1) (1) (1) (1) (1) (1) (1) (4) (4) (1) (1) (1) (1) (1) (1) (2) (6) (3) (1).xlsx to kd_master_new_updated (2) (1) (7) (1) (2) (1) (5) (1) (1) (1) (1) (1) (1) (1) (1) (4) (4) (1) (1) (1) (1) (1) (1) (2) (6) (3) (1).xlsx\n",
            "\n",
            "Enter NEW overlay files (CSV/XLSX); blank to stop.\n",
            " Already have: genes_clean_expanded.csv, EpigenPandas.xlsx, https://pmc.ncbi.nlm.nih.gov/articles/instance/3326523/bin/mmc4.pdf, table_s6_full.xlsx, heatmap_matrix_output_cleaned_corrected.xlsx, KdAdditions.xlsx, Structural Basis for Acetylated Histone H4 Recognition by the Human BRD2 Broodomain.xlsx, Specificity of the HP1 chromo domain for the methylated N‚Äêterminus of histone H3.xlsx, Selective recognition of methylated lysine 9 on histone H3 by the HP1 chromo domain.xlsx, Progress in the Discovery of Small-Molecule Inhibitors of Bromodomain‚ÄìHistone Interactions.xlsx, Improved methods for the detection of histone interactions with peptide microarrays.xlsx, Histone peptide microarray screen of chromo and Tudor domains defines new histone lysine methylation interactions.xlsx, Assays for the Determination of Structure and Dynamics of the Interaction of the Chromodomain with Histone Peptides.xlsx, Figure 1 Master Compilation With Labels.xlsx\n",
            "Overlay file: \n",
            "\n",
            "Manual Kd entry ‚Äì blank reader to finish.\n",
            "Reader: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1565898765.py:202: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  matrix_parsed = matrix.applymap(parse_tuple_cell_to_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(Optional) Upload a BASELINE workbook (has the original (0, 11, strength) triples) to rescue Source 11:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2f6bea52-c941-445b-b845-a20f2b97a1b4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2f6bea52-c941-445b-b845-a20f2b97a1b4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kd_master_new_updated (2) (1) (7) (1) (2) (1) (5) (1) (1).xlsx to kd_master_new_updated (2) (1) (7) (1) (2) (1) (5) (1) (1).xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1565898765.py:262: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  rescue_parsed = rescue_df.applymap(parse_tuple_cell_to_list)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ S11 rescue complete. Added S11 to empty cells: 0; to non-empty cells: 2328; Luke pairs added: 5299\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'DNA motif'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H1.4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2B'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3 and H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2Aac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2Bac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3ac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4Kac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2B'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2B'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'DNA motif'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2B'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'HKme'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2AX'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H1'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2B'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'DNA motif'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2B'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2Aac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H2Bac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3ac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H4ac'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3R2'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'mCG'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'mCG'.\n",
            "‚è≠Ô∏è Skipping denylisted category-like residue 'H3'.\n",
            "\n",
            "Upload Luke's wide file (optional):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9d8ee687-feec-4591-82ef-ad0f37cdee28\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9d8ee687-feec-4591-82ef-ad0f37cdee28\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Figure 1 Master Compilation With Labels.xlsx to Figure 1 Master Compilation With Labels.xlsx\n",
            "Luke sheet detection: {'PE': 'PE percent positive', 'Gate': 'Accounting for Gating ', 'N': 'Number of Reader Expressing Cel'}\n",
            "Luke integration: rows=38, cols=167, appended pairs=364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1565898765.py:718: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  matrix_parsed = matrix_parsed.applymap(_dedupe_numeric_pairs)\n",
            "/tmp/ipython-input-1565898765.py:883: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  has_any = df_kd[_reader_cols].applymap(_cell_has_payload).any(axis=1)\n",
            "/tmp/ipython-input-1565898765.py:904: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  has_any_data  = df_kd[_reader_cols].applymap(_cell_has_payload).any(axis=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Removing 16 placeholder rows with no data: DNA motif, H1, H1.4, H2AX, H2Aac, H2B, H2Bac, H3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1565898765.py:1161: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  s11_only = matrix_parsed.applymap(lambda lst: _only_source_key_triples(lst, 11))\n",
            "/tmp/ipython-input-1565898765.py:1240: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  df_kd.set_index(\"Histone residues\").to_excel(w, \"Kd_matrix_indexed\")\n",
            "/tmp/ipython-input-1565898765.py:1241: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  df_kd_my_only.set_index(\"Histone residues\").to_excel(w, \"Kd_matrix_indexed_MY_ONLY\")\n",
            "/tmp/ipython-input-1565898765.py:1245: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  refs_df.to_excel(w, \"References\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1246: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  keys_df.to_excel(w, \"Keys\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1247: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  index_keys_df.to_excel(w, \"Index Keys\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1248: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  mod_index_df.to_excel(w, \"Mod Indexes\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1249: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  hist_index_df.to_excel(w, \"Histone number indexes\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1250: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  site_index_df.to_excel(w, \"Histone site indexes\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1254: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  }).to_excel(w, \"Kd Index Keys\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1255: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  s11_df_out.set_index(\"Histone residues\").to_excel(w, \"Source 11\")\n",
            "/tmp/ipython-input-1565898765.py:1262: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  .to_excel(w, \"Luke's paper\"))\n",
            "/tmp/ipython-input-1565898765.py:1264: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  lukes_plus_df_out.to_excel(w, \"Luke's paper plus\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1268: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  lukes_plus_hits_df_out.to_excel(w, \"Luke's paper plus hits\", index=False)\n",
            "/tmp/ipython-input-1565898765.py:1271: FutureWarning: Starting with pandas version 3.0 all arguments of to_excel except for the argument 'excel_writer' will be keyword-only.\n",
            "  _sequences_df.to_excel(w, \"Sequences\", index=False)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_435a07a2-fc11-49b2-838d-fb37a4df4c82\", \"kd_master_new_updated (2) (1) (7) (1) (2) (1) (5) (1) (1) (1) (1) (1) (1) (1) (1) (4) (4) (1) (1) (1) (1) (1) (1) (2) (6) (3) (1).xlsx\", 1156935)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Auto-download started for: kd_master_new_updated (2) (1) (7) (1) (2) (1) (5) (1) (1) (1) (1) (1) (1) (1) (1) (4) (4) (1) (1) (1) (1) (1) (1) (2) (6) (3) (1).xlsx\n",
            "\n",
            "üîé Interactive lookup:\n",
            "   (n) name pair   (i) index pair   (s) search names   (q) quit\n",
            "Lookup mode [n/i/s/q]: q\n",
            "\n",
            "‚úÖ Done. Sheets written:\n",
            " - 'Luke's paper plus' (quartiles based on pos4 > 0.0; blanks/‚â§0 => 0)\n",
            " - 'Luke's paper plus hits' (thresholded: pos4 < 0.15 => 0; pos4 ‚â• 0.15 quartiled into 1‚Äì4)\n",
            " - 'Kd_matrix_indexed' now fills pos3 with Luke hit-buckets ONLY when pos3 was NA/blank (no overwrites).\n",
            " - 'Mod Indexes', 'Histone number indexes', 'Histone site indexes' are rebuilt to reflect newly observed data.\n",
            " - 'Sequences' restored (preserves existing sequences if present; adds missing histones/readers).\n"
          ]
        }
      ],
      "source": [
        "# Colab Cell: Full pipeline with KD-string fix + numeric‚Üístrength mapping + correct Luke handling\n",
        "#             + ADDITIVE S11 rescue + lookup\n",
        "# Key points:\n",
        "# - Keep legacy Source 11 (11). Never delete/remap (0,11, ...).\n",
        "# - Fuzzy 0..4 strength parsing (strings/floats) AND 0..4 from numeric Kd strings (¬± handled; default unit = uM).\n",
        "# - Never confuse presence (0,key) with Luke numeric (delta,n).\n",
        "# - Prefer Luke's key as source only when *only* Luke numeric is present (no KD/strength).\n",
        "# - S11 Rescue: *additively* re-inject dropped (0,11,strength) + Luke numeric from a baseline workbook.\n",
        "# - Heatmap guard: never auto-fill kd_idx_auto for heatmap rows.\n",
        "# - One-time cleanup of blank/'nan' rows without data + targeted placeholder rows without payload.\n",
        "# - Interactive lookup.\n",
        "#\n",
        "# ADDITIONS IN THIS VERSION:\n",
        "# - NEW sheet \"Luke's paper plus hits\" that applies delta>=0.15 as \"binding\":\n",
        "#     * delta < 0.15 or blank => Bucket 0\n",
        "#     * delta >= 0.15 => quartiles into Buckets 1-4 based ONLY on those >= 0.15\n",
        "# - Propagate those new bucket strengths into Kd_matrix_indexed tuples (pos3) WHEN pos3 is NA/blank,\n",
        "#   so Luke-derived interactions carry their strength into the matrix without overwriting existing pos3.\n",
        "# - Ensure Mod Indexes updates when new mod tags appear in residues: discover missing tags and extend MOD_INDEX.\n",
        "#\n",
        "# ADDITIONS IN THIS VERSION (RE-ADD):\n",
        "# - Re-add \"Sequences\" sheet that lists general histones + all reader names with a blank Sequence column.\n",
        "#   If an existing \"Sequences\" sheet exists, preserve existing sequences and only add missing entries.\n",
        "\n",
        "import re, ast, pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "# ---------- 1) Upload workbook ----------\n",
        "print(\"üîÑ Upload your latest kd_master .xlsx:\")\n",
        "uploaded = files.upload()\n",
        "kd_file = Path(next(iter(uploaded)))\n",
        "xls     = pd.ExcelFile(kd_file)\n",
        "\n",
        "# ---------- 2) Load core sheets ----------\n",
        "orig_df       = pd.read_excel(xls, \"Kd_matrix_indexed\", dtype=str)\n",
        "refs_df       = pd.read_excel(xls, \"References\")\n",
        "keys_existing = pd.read_excel(xls, \"Keys\")\n",
        "\n",
        "keys_existing.columns = (\n",
        "    keys_existing.columns.str.strip()\n",
        "        .str.lower()\n",
        "        .str.replace(\" \", \"_\")\n",
        ")\n",
        "\n",
        "existing_meta = {}\n",
        "for _, r in keys_existing.iterrows():\n",
        "    for fn in re.split(r\"[;,]\\s*\", str(r.filename)):\n",
        "        fn = fn.strip()\n",
        "        if not fn: continue\n",
        "        existing_meta[fn] = {\n",
        "            \"source_name\":        r.source_name,\n",
        "            \"URL\":                r.url,\n",
        "            \"measurement_method\": r.measurement_method\n",
        "        }\n",
        "\n",
        "# ---------- 3) Overlay loader ----------\n",
        "print(\"\\nEnter NEW overlay files (CSV/XLSX); blank to stop.\")\n",
        "print(\" Already have:\", \", \".join(existing_meta.keys()) or \"none\")\n",
        "overlay_paths = []\n",
        "while True:\n",
        "    p = input(\"Overlay file: \").strip()\n",
        "    if not p: break\n",
        "    if p in existing_meta:\n",
        "        print(\" ‚Üí Skipping already-loaded:\", p)\n",
        "    else:\n",
        "        overlay_paths.append(Path(p))\n",
        "\n",
        "def _norm(df):\n",
        "    m = {}\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        if \"reader\"  in lc:   m[c] = \"Histone readers\"\n",
        "        elif \"residue\" in lc: m[c] = \"Histone residues\"\n",
        "        elif \"kd\"      in lc: m[c] = \"Kd\"\n",
        "        elif \"ref\"     in lc: m[c] = \"Reference\"\n",
        "        else:                 m[c] = c\n",
        "    return df.rename(columns=m).rename_axis(None, axis=1)\n",
        "\n",
        "for ov in overlay_paths:\n",
        "    df0 = pd.read_csv(ov) if ov.suffix.lower()==\".csv\" else pd.read_excel(ov)\n",
        "    df1 = _norm(df0)\n",
        "    rows = [{\n",
        "        \"Histone readers\":  str(r.get(\"Histone readers\",\"\")).strip(),\n",
        "        \"Histone residues\": str(r.get(\"Histone residues\",\"\")).strip(),\n",
        "        \"Kd\":               str(r.get(\"Kd\",\"\")).strip(),\n",
        "        \"Reference\":        (str(r.get(\"Reference\",\"\")).strip() or ov.name)\n",
        "    } for _, r in df1.iterrows()]\n",
        "    refs_df = pd.concat([refs_df, pd.DataFrame(rows)], ignore_index=True)\n",
        "\n",
        "# ---------- 5) Manual Kd entry ----------\n",
        "print(\"\\nManual Kd entry ‚Äì blank reader to finish.\")\n",
        "while True:\n",
        "    rdr = input(\"Reader: \").strip()\n",
        "    if not rdr: break\n",
        "    res = input(\"Residue: \").strip()\n",
        "    kd  = input(\"Kd: \").strip()\n",
        "    rf  = input(\"Reference/URL/DOI: \").strip() or \"manual\"\n",
        "    refs_df = pd.concat([refs_df, pd.DataFrame([{\n",
        "        \"Histone readers\":  rdr,\n",
        "        \"Histone residues\": res,\n",
        "        \"Kd\":               kd,\n",
        "        \"Reference\":        rf\n",
        "    }])], ignore_index=True)\n",
        "\n",
        "# ---------- 6) Clean refs ----------\n",
        "refs_df[\"Histone readers\"]  = refs_df[\"Histone readers\"].astype(str).str.replace(r\"\\(.*\\)\",\"\",regex=True).str.strip()\n",
        "refs_df[\"Histone residues\"] = refs_df[\"Histone residues\"].astype(str).str.strip()\n",
        "refs_df[\"Kd\"]               = refs_df[\"Kd\"].astype(str).str.strip()\n",
        "refs_df[\"Reference\"]        = refs_df[\"Reference\"].astype(str).str.strip()\n",
        "\n",
        "bad_mask = refs_df[\"Histone residues\"].str.len().eq(0) | refs_df[\"Histone residues\"].isin({\"#\", \"nan\", \"NaN\", \"None\"})\n",
        "if bad_mask.any():\n",
        "    print(f\"‚ÑπÔ∏è  Skipping {bad_mask.sum()} rows with invalid residue labels (#/empty/nan).\")\n",
        "refs_df = refs_df[~bad_mask].drop_duplicates([\"Histone readers\",\"Histone residues\",\"Kd\",\"Reference\"]).reset_index(drop=True)\n",
        "\n",
        "# --- Heatmap guard: never auto-fill kd_idx_auto for heatmap rows\n",
        "HEATMAP_REF = \"heatmap_matrix_output_cleaned_corrected.xlsx\"\n",
        "if \"kd_idx_auto\" not in refs_df.columns:\n",
        "    refs_df[\"kd_idx_auto\"] = np.nan\n",
        "mask_heatmap = refs_df[\"Reference\"].astype(str).str.strip().eq(HEATMAP_REF)\n",
        "refs_df.loc[mask_heatmap, \"kd_idx_auto\"] = np.nan\n",
        "\n",
        "# ---------- 7) Build/refresh Keys ----------\n",
        "raw_refs = list(refs_df[\"Reference\"].unique())\n",
        "all_keys = []\n",
        "for ref in raw_refs:\n",
        "    if ref in existing_meta:\n",
        "        m = existing_meta[ref]\n",
        "        all_keys.append({\n",
        "            \"filename\":           ref,\n",
        "            \"source_name\":        m[\"source_name\"],\n",
        "            \"URL\":                m[\"URL\"],\n",
        "            \"measurement_method\": m[\"measurement_method\"]\n",
        "        })\n",
        "    else:\n",
        "        src = input(f\"Source name for {ref}: \").strip() or ref\n",
        "        url = input(f\"URL for {ref}: \").strip()\n",
        "        mm  = input(f\"Measurement for {ref}: \").strip()\n",
        "        all_keys.append({\n",
        "            \"filename\":           ref,\n",
        "            \"source_name\":        src,\n",
        "            \"URL\":                url,\n",
        "            \"measurement_method\": mm\n",
        "        })\n",
        "\n",
        "grouped = {}\n",
        "for k in all_keys:\n",
        "    key = str(k[\"source_name\"]).strip().lower()\n",
        "    grp = grouped.setdefault(key, {\n",
        "        \"source_name\": k[\"source_name\"],\n",
        "        \"filenames\":   [], \"urls\": [], \"methods\": []\n",
        "    })\n",
        "    grp[\"filenames\"].append(k[\"filename\"])\n",
        "    grp[\"urls\"].append(k[\"URL\"])\n",
        "    grp[\"methods\"].append(k[\"measurement_method\"])\n",
        "\n",
        "entries = []\n",
        "for i, grp in enumerate(grouped.values(), start=1):\n",
        "    entries.append({\n",
        "        \"key\":                i,\n",
        "        \"filename\":           \" , \".join(dict.fromkeys(map(str, grp[\"filenames\"]))),\n",
        "        \"source_name\":        grp[\"source_name\"],\n",
        "        \"URL\":                \" , \".join(map(str, dict.fromkeys(grp[\"urls\"]))),\n",
        "        \"measurement_method\": \" , \".join(map(str, dict.fromkeys(grp[\"methods\"])) )\n",
        "    })\n",
        "keys_df = pd.DataFrame(entries)\n",
        "\n",
        "ref_to_key = {}\n",
        "for _, row in keys_df.iterrows():\n",
        "    for fn in str(row[\"filename\"]).split(\" , \"):\n",
        "        fn = fn.strip()\n",
        "        if fn:\n",
        "            ref_to_key[fn] = row[\"key\"]\n",
        "\n",
        "# ---------- 8) Read Kd matrix and parse tuple strings ----------\n",
        "raw_mat = pd.read_excel(xls, \"Kd_matrix_indexed\", dtype=str)\n",
        "matrix = raw_mat.set_index(\"Histone residues\") if \"Histone residues\" in raw_mat.columns else raw_mat\n",
        "matrix.index = matrix.index.map(lambda x: \"\" if pd.isna(x) else str(x).strip())\n",
        "matrix = matrix.loc[:, [c for c in matrix.columns if not str(c).lower().startswith(\"unnamed\")]]\n",
        "\n",
        "def parse_tuple_cell_to_list(cell):\n",
        "    if cell in (None, np.nan): return []\n",
        "    s = str(cell).strip()\n",
        "    if not s or s == \"(0,)\": return []\n",
        "    out = []\n",
        "    for part in [p.strip() for p in s.split(\";\") if p.strip()]:\n",
        "        if not (part.startswith(\"(\") and part.endswith(\")\")):\n",
        "            continue\n",
        "        try:\n",
        "            tup = ast.literal_eval(part)\n",
        "            if isinstance(tup, tuple):\n",
        "                out.append(tup)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "def list_to_tuple_cell(lst):\n",
        "    if not lst: return \"(0,)\"\n",
        "    return \"; \".join(\"(\" + \", \".join(repr(x) for x in t) + \")\" for t in lst)\n",
        "\n",
        "matrix_parsed = matrix.applymap(parse_tuple_cell_to_list)\n",
        "\n",
        "# NEW: helper used in 8b and 11\n",
        "def _valid_residue_label(x):\n",
        "    if pd.isna(x):\n",
        "        return False\n",
        "    s = str(x).strip()\n",
        "    return s != \"\" and s.lower() != \"nan\"\n",
        "\n",
        "# ---------- 8b) S11 RESCUE (ADDITIVE) ----------\n",
        "from google.colab import files as _files_rescue\n",
        "\n",
        "def _is_presence_pair(t):\n",
        "    return (isinstance(t, tuple) and len(t)==2 and t[0]==0\n",
        "            and isinstance(t[1], (int,float)) and float(t[1]).is_integer())\n",
        "\n",
        "def _strength_from_any(x):\n",
        "    _WORDS = {\n",
        "        \"no spot\":0,\"nospot\":0,\"none\":0,\"0\":0,\"0.0\":0,\n",
        "        \"very weak\":1,\"weak\":1,\"1\":1,\"1.0\":1,\n",
        "        \"moderate\":2,\"mod\":2,\"2\":2,\"2.0\":2,\n",
        "        \"strong\":3,\"3\":3,\"3.0\":3,\n",
        "        \"very strong\":4,\"vstrong\":4,\"4\":4,\"4.0\":4,\n",
        "    }\n",
        "    if isinstance(x, (int, float)):\n",
        "        v = int(round(float(x)))\n",
        "        return v if 0 <= v <= 4 else None\n",
        "    s = str(x).strip().lower()\n",
        "    if s in _WORDS: return _WORDS[s]\n",
        "    m = re.search(r'\\b([0-4])(?:\\.0+)?\\b', s.replace(\",\", \" \"))\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _find_luke_pair(items):\n",
        "    for t in items if isinstance(items, list) else []:\n",
        "        if isinstance(t, tuple) and len(t)==2 and all(isinstance(x,(int,float)) for x in t):\n",
        "            if _is_presence_pair(t):  # skip (0, key)\n",
        "                continue\n",
        "            v, n = t\n",
        "            return (float(v), int(n))\n",
        "    return None\n",
        "\n",
        "def _has_s11_triple(items, s11_key=11):\n",
        "    if not isinstance(items, list): return False\n",
        "    for t in items:\n",
        "        if isinstance(t, tuple) and len(t)>=3:\n",
        "            try:\n",
        "                if t[0]==0 and int(t[1])==int(s11_key):\n",
        "                    return True\n",
        "            except Exception:\n",
        "                pass\n",
        "    return False\n",
        "\n",
        "print(\"\\n(Optional) Upload a BASELINE workbook (has the original (0, 11, strength) triples) to rescue Source 11:\")\n",
        "_rescue = _files_rescue.upload()\n",
        "if _rescue:\n",
        "    rescue_path = Path(next(iter(_rescue)))\n",
        "    rescue_xls  = pd.ExcelFile(rescue_path)\n",
        "    rescue_df   = pd.read_excel(rescue_xls, \"Kd_matrix_indexed\", dtype=str)\n",
        "    rescue_df   = rescue_df.set_index(\"Histone residues\")\n",
        "    rescue_df   = rescue_df.loc[:, [c for c in rescue_df.columns if not str(c).lower().startswith(\"unnamed\")]]\n",
        "    rescue_parsed = rescue_df.applymap(parse_tuple_cell_to_list)\n",
        "\n",
        "    s11_key = 11\n",
        "    added_s11_to_empty = added_s11_to_nonempty = added_luke_pairs = 0\n",
        "\n",
        "    # Ensure all readers/cols exist\n",
        "    for rdr in rescue_parsed.columns:\n",
        "        if rdr not in matrix_parsed.columns:\n",
        "            matrix_parsed[rdr] = [list() for _ in range(len(matrix_parsed.index))]\n",
        "\n",
        "    # Only add rows with a real, non-empty label\n",
        "    need_rows = [r for r in rescue_parsed.index\n",
        "                 if _valid_residue_label(r) and r not in matrix_parsed.index]\n",
        "    if need_rows:\n",
        "        add = pd.DataFrame({c: [list() for _ in range(len(need_rows))] for c in matrix_parsed.columns},\n",
        "                           index=need_rows)\n",
        "        matrix_parsed = pd.concat([matrix_parsed, add], axis=0)\n",
        "\n",
        "    # Additive merge of S11 triples and Luke numeric pairs\n",
        "    for res in rescue_parsed.index:\n",
        "        for rdr in rescue_parsed.columns:\n",
        "            old_items = rescue_parsed.at[res, rdr]\n",
        "            if not isinstance(old_items, list) or len(old_items)==0:\n",
        "                continue\n",
        "\n",
        "            # strongest S11 strength in baseline\n",
        "            s11_strength = None\n",
        "            for t in old_items:\n",
        "                if isinstance(t, tuple) and len(t)>=3 and t[0]==0:\n",
        "                    try: key = int(t[1])\n",
        "                    except Exception: continue\n",
        "                    if key == s11_key:\n",
        "                        st = _strength_from_any(t[2])\n",
        "                        if st is not None and (s11_strength is None or st > s11_strength):\n",
        "                            s11_strength = st\n",
        "\n",
        "            luke_pair = _find_luke_pair(old_items)\n",
        "\n",
        "            now_items = matrix_parsed.at[res, rdr]\n",
        "            if not isinstance(now_items, list):\n",
        "                now_items = []\n",
        "            had_any = len(now_items) > 0\n",
        "            had_s11 = _has_s11_triple(now_items, s11_key=s11_key)\n",
        "\n",
        "            if s11_strength is not None and not had_s11:\n",
        "                now_items.append((0, s11_key, s11_strength))\n",
        "                if had_any: added_s11_to_nonempty += 1\n",
        "                else:       added_s11_to_empty    += 1\n",
        "\n",
        "            if luke_pair is not None:\n",
        "                already_have_pair = any(\n",
        "                    (isinstance(t, tuple) and len(t)==2 and not _is_presence_pair(t) and\n",
        "                     abs(float(t[0]) - float(luke_pair[0])) < 1e-9 and int(t[1]) == int(luke_pair[1]))\n",
        "                    for t in now_items\n",
        "                )\n",
        "                if not already_have_pair:\n",
        "                    now_items.append((round(float(luke_pair[0]), 4), int(luke_pair[1])))\n",
        "                    added_luke_pairs += 1\n",
        "\n",
        "            matrix_parsed.at[res, rdr] = now_items\n",
        "\n",
        "    print(f\"‚úÖ S11 rescue complete. Added S11 to empty cells: {added_s11_to_empty}; \"\n",
        "          f\"to non-empty cells: {added_s11_to_nonempty}; Luke pairs added: {added_luke_pairs}\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping S11 rescue (no baseline uploaded).\")\n",
        "\n",
        "# ---------- 9) Residue canonicalization ----------\n",
        "def canonicalize_residue_for_matrix(raw_res, idx):\n",
        "    r = str(raw_res).strip()\n",
        "    if not r or r.lower() in {\"nan\", \"#\"}:\n",
        "        return None\n",
        "    r = re.sub(r\"\\s*\\(.*\\)\", \"\", r)\n",
        "    r = re.sub(r\"\\s+\", \"\", r)\n",
        "    if r in idx:\n",
        "        return r\n",
        "    m = re.match(r'^(H2)K(\\d+)([A-Za-z0-9]+)$', r, flags=re.I)\n",
        "    if m:\n",
        "        site, mod = m.group(2), m.group(3)\n",
        "        candidates = [f\"H2AK{site}{mod}\", f\"H2BK{site}{mod}\"]\n",
        "        available  = [c for c in candidates if any(str(x).startswith(c) for x in idx)]\n",
        "        if len(available) == 1:\n",
        "            return available[0]\n",
        "        if len(available) > 1:\n",
        "            print(f\"‚ùì Ambiguous residue '{r}'. Options:\")\n",
        "            for i, c in enumerate(available, 1): print(f\"   {i}) {c}\")\n",
        "            sel = input(\"   Choose 1/2 (blank to skip): \").strip()\n",
        "            if sel.isdigit() and 1 <= int(sel) <= len(available):\n",
        "                return available[int(sel)-1]\n",
        "            return None\n",
        "    matches = [x for x in idx if str(x).startswith(r)]\n",
        "    if len(matches) == 1:\n",
        "        return matches[0]\n",
        "    return None\n",
        "\n",
        "# ---------- S11 key (keep legacy 11 intact) ----------\n",
        "S11_NAME = \"Histone Recognition and Large-Scale Structural Analysis of the Human Bromodomain Family\"\n",
        "s11_key = 11  # do NOT remap legacy 11\n",
        "\n",
        "# ---------- 10) Merge overlay refs into matrix_parsed ----------\n",
        "DENYLIST_NON_HISTONE = {\n",
        "    \"dnamotif\",\"h1\",\"h1.4\",\"h2ax\",\"h2aac\",\"h2b\",\"h2bac\",\"h3\",\"h3andh4\",\n",
        "    \"h3r2\",\"h3ac\",\"h4\",\"h4kac\",\"h4ac\",\"hkme\",\"mcg\"\n",
        "}\n",
        "\n",
        "for _, row in refs_df.iterrows():\n",
        "    rdr     = row[\"Histone readers\"]\n",
        "    raw_res = str(row[\"Histone residues\"])\n",
        "    raw_res_norm = re.sub(r\"\\s+\", \"\", raw_res).strip().lower()\n",
        "\n",
        "    if raw_res_norm in DENYLIST_NON_HISTONE:\n",
        "        print(f\"‚è≠Ô∏è Skipping denylisted category-like residue '{raw_res}'.\")\n",
        "        continue\n",
        "\n",
        "    canon   = canonicalize_residue_for_matrix(raw_res, matrix_parsed.index)\n",
        "    if not canon:\n",
        "        print(f\"‚ö†Ô∏è  Residue '{raw_res}' not found/ambiguous; skipping.\")\n",
        "        continue\n",
        "    if rdr not in matrix_parsed.columns:\n",
        "        matrix_parsed[rdr] = [list() for _ in range(len(matrix_parsed.index))]\n",
        "    ref_name = row[\"Reference\"]\n",
        "    if ref_name not in ref_to_key:\n",
        "        print(f\"‚ö†Ô∏è  Reference '{ref_name}' has no key; skipping this entry.\")\n",
        "        continue\n",
        "    k = ref_to_key[ref_name]\n",
        "    raw = str(row[\"Kd\"]).strip().lower()\n",
        "    if not raw or raw==\"nan\":\n",
        "        continue\n",
        "    tri = (0, k, {\"No Spot\":0, \"no spot\":0, \"very weak\":1, \"moderate\":2, \"strong\":3, \"very strong\":4}.get(raw, row[\"Kd\"]))\n",
        "    base = matrix_parsed.at[canon, rdr]\n",
        "    if tri not in base:\n",
        "        base.append(tri)\n",
        "\n",
        "# ---------- 11) OPTIONAL: Upload Luke file and integrate ----------\n",
        "from google.colab import files as _files_optional\n",
        "\n",
        "def _canon_label(s: str) -> str:\n",
        "    if pd.isna(s): return \"\"\n",
        "    return re.sub(r\"\\s+\", \"\", str(s)).strip()\n",
        "\n",
        "def _is_residue_like(s: str) -> bool:\n",
        "    t = str(s).lower()\n",
        "    return (\"h\" in t) and any(ch.isdigit() for ch in t)\n",
        "\n",
        "def _promote_header_if_unnamed(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    unnamed = sum(str(c).startswith(\"Unnamed\") for c in df.columns)\n",
        "    if len(df.columns) and unnamed / len(df.columns) > 0.5 and len(df) > 1:\n",
        "        new_header = df.iloc[0].astype(str).tolist()\n",
        "        df = df.iloc[1:].copy()\n",
        "        df.columns = new_header\n",
        "    return df\n",
        "\n",
        "def _coerce_percent_to_float(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s = str(x).strip().replace(\",\", \"\")\n",
        "    if not s: return np.nan\n",
        "    if s.endswith(\"%\"):\n",
        "        s = s[:-1]\n",
        "        try: return float(s)/100.0\n",
        "        except: return np.nan\n",
        "    try:\n",
        "        v = float(s)\n",
        "        return v if 0 <= v <= 1.2 else v/100.0\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "def _fuzzy_sheet(xls: pd.ExcelFile, targets):\n",
        "    names = list(xls.sheet_names)\n",
        "    low   = [n.strip().lower() for n in names]\n",
        "    tgt   = [t.strip().lower() for t in targets]\n",
        "    for t in tgt:\n",
        "        if t in low:\n",
        "            return names[low.index(t)]\n",
        "    for t in tgt:\n",
        "        for i, nm in enumerate(low):\n",
        "            if nm.startswith(t) or t in nm:\n",
        "                return names[i]\n",
        "    return None\n",
        "\n",
        "def _detect_residue_col(df: pd.DataFrame, default_col_idx: int) -> int:\n",
        "    best_j, best_hits = default_col_idx, -1\n",
        "    for j, c in enumerate(df.columns):\n",
        "        series = df.iloc[:, j].astype(str)\n",
        "        hits = (series.map(_is_residue_like)).sum()\n",
        "        if hits > best_hits:\n",
        "            best_hits, best_j = hits, j\n",
        "    return best_j\n",
        "\n",
        "def _longify(df: pd.DataFrame, default_col_idx: int, sheet_tag: str):\n",
        "    df = df.dropna(how=\"all\").dropna(axis=1, how=\"all\")\n",
        "    df = _promote_header_if_unnamed(df)\n",
        "    residue_col_idx  = _detect_residue_col(df, default_col_idx)\n",
        "    residue_col_name = df.columns[residue_col_idx]\n",
        "    reader_cols = [c for c in df.columns if c != residue_col_name]\n",
        "    tidy = df[[residue_col_name] + reader_cols].copy()\n",
        "    tidy.columns = [\"Residue\"] + [str(c).strip() for c in reader_cols]\n",
        "    tidy = tidy.melt(id_vars=[\"Residue\"], var_name=\"Reader\", value_name=sheet_tag)\n",
        "    tidy[\"Residue\"] = tidy[\"Residue\"].astype(str).map(_canon_label)\n",
        "    tidy[\"Reader\"]  = tidy[\"Reader\"].astype(str).str.strip()\n",
        "    return tidy\n",
        "\n",
        "print(\"\\nUpload Luke's wide file (optional):\")\n",
        "_uploaded_opt = _files_optional.upload()\n",
        "luke_filename = None\n",
        "if _uploaded_opt:\n",
        "    luke_filename = Path(next(iter(_uploaded_opt)))\n",
        "\n",
        "k_luke = None\n",
        "luke_sheet_df_wide = None\n",
        "\n",
        "if luke_filename:\n",
        "    luke_xls = pd.ExcelFile(luke_filename)\n",
        "    pe_name   = _fuzzy_sheet(luke_xls, [\"PE percent positive\", \"pe percent positive\", \"pe%\"])\n",
        "    gate_name = _fuzzy_sheet(luke_xls, [\"Accounting for Gating\", \"accounting for gating\"])\n",
        "    n_name    = _fuzzy_sheet(luke_xls, [\"Number of Reader Expressing Cells\", \"number of reader expressing cel\"])\n",
        "    print(\"Luke sheet detection:\", {\"PE\":pe_name, \"Gate\":gate_name, \"N\":n_name})\n",
        "\n",
        "    if not (pe_name and gate_name and n_name):\n",
        "        print(\"Luke: required sheets not all found; skipping Luke merge.\")\n",
        "    else:\n",
        "        pe_df   = pd.read_excel(luke_xls, sheet_name=pe_name,   dtype=str)\n",
        "        gate_df = pd.read_excel(luke_xls, sheet_name=gate_name, dtype=str)\n",
        "        n_df    = pd.read_excel(luke_xls, sheet_name=n_name,    dtype=str)\n",
        "\n",
        "        pe_long   = _longify(pe_df,   1, \"pe\")\n",
        "        gate_long = _longify(gate_df, 0, \"gate\")\n",
        "        n_long    = _longify(n_df,    0, \"n\")\n",
        "\n",
        "        pe_long[\"pe\"]     = pe_long[\"pe\"].map(_coerce_percent_to_float)\n",
        "        gate_long[\"gate\"] = gate_long[\"gate\"].map(_coerce_percent_to_float)\n",
        "        def _coerce_int(x):\n",
        "            if pd.isna(x): return np.nan\n",
        "            try: return int(float(str(x).replace(\",\", \"\")))\n",
        "            except: return np.nan\n",
        "        n_long[\"n\"] = n_long[\"n\"].map(_coerce_int)\n",
        "\n",
        "        lg = (pe_long\n",
        "              .merge(gate_long, on=[\"Residue\",\"Reader\"], how=\"outer\")\n",
        "              .merge(n_long,    on=[\"Residue\",\"Reader\"], how=\"outer\"))\n",
        "\n",
        "        lg[\"delta\"]  = (lg[\"pe\"] - lg[\"gate\"]).clip(lower=0)\n",
        "        lg[\"is_hit\"] = (lg[\"delta\"] >= 0.15)\n",
        "        lg_use = lg[(~lg[\"pe\"].isna()) & (~lg[\"gate\"].isna()) & (lg[\"n\"] >= 1000)].copy()\n",
        "\n",
        "        def _fmt(delta, n, hit):\n",
        "            if pd.isna(delta) or pd.isna(n): return \"\"\n",
        "            s = f\"{delta:.4f} (n={int(n)})\"\n",
        "            return s + \"*\" if bool(hit) else s\n",
        "\n",
        "        luke_wide = (lg_use\n",
        "                     .assign(val=lambda d: d.apply(lambda r: _fmt(r[\"delta\"], r[\"n\"], r[\"is_hit\"]), axis=1))\n",
        "                     .pivot_table(index=\"Residue\", columns=\"Reader\", values=\"val\", aggfunc=\"first\")\n",
        "                     .sort_index())\n",
        "\n",
        "        luke_sheet_df_wide = luke_wide.copy()\n",
        "\n",
        "        # Ensure Luke exists in Keys and get numeric key\n",
        "        luke_source_name = \"Luke's paper\"\n",
        "        luke_method      = \"Cell-based PE‚àígate\"\n",
        "        if not (keys_df[\"source_name\"].astype(str) == luke_source_name).any():\n",
        "            next_key = (pd.to_numeric(keys_df[\"key\"], errors=\"coerce\").max() or 0) + 1\n",
        "            keys_df = pd.concat([keys_df, pd.DataFrame([{\n",
        "                \"key\": next_key,\n",
        "                \"filename\": str(luke_filename.name),\n",
        "                \"source_name\": luke_source_name,\n",
        "                \"URL\": \"\",\n",
        "                \"measurement_method\": luke_method\n",
        "            }])], ignore_index=True)\n",
        "\n",
        "        # rebuild filename‚Üíkey\n",
        "        ref_to_key = {}\n",
        "        for _, row in keys_df.iterrows():\n",
        "            for fn in str(row[\"filename\"]).split(\" , \"):\n",
        "                fn = fn.strip()\n",
        "                if fn:\n",
        "                    ref_to_key[fn] = row[\"key\"]\n",
        "        k_luke = ref_to_key.get(str(luke_filename.name))\n",
        "\n",
        "        # Ensure readers present in matrix (for tuple storage)\n",
        "        for rdr in luke_wide.columns:\n",
        "            if rdr not in matrix_parsed.columns:\n",
        "                matrix_parsed[rdr] = [list() for _ in range(len(matrix_parsed.index))]\n",
        "\n",
        "        # Make sure rows exist if we later append tuples\n",
        "        need_rows = [r for r in luke_wide.index\n",
        "                     if _valid_residue_label(r) and r not in matrix_parsed.index]\n",
        "        if need_rows:\n",
        "            add = pd.DataFrame({c: [list() for _ in range(len(need_rows))] for c in matrix_parsed.columns},\n",
        "                               index=need_rows)\n",
        "            matrix_parsed = pd.concat([matrix_parsed, add], axis=0)\n",
        "\n",
        "        appended = 0\n",
        "for res in luke_wide.index:\n",
        "    if (not _valid_residue_label(res)) or (res not in matrix_parsed.index):\n",
        "        continue\n",
        "\n",
        "for rdr in luke_wide.columns:\n",
        "    if rdr not in matrix_parsed.columns:\n",
        "        matrix_parsed[rdr] = [list() for _ in range(len(matrix_parsed.index))]\n",
        "\n",
        "need_rows = [r for r in luke_wide.index\n",
        "             if _valid_residue_label(r) and r not in matrix_parsed.index]\n",
        "if need_rows:\n",
        "    add = pd.DataFrame({c: [list() for _ in range(len(need_rows))] for c in matrix_parsed.columns},\n",
        "                       index=need_rows)\n",
        "    matrix_parsed = pd.concat([matrix_parsed, add], axis=0)\n",
        "\n",
        "appended = 0\n",
        "\n",
        "for res in luke_wide.index:\n",
        "    if (not _valid_residue_label(res)) or (res not in matrix_parsed.index):\n",
        "        continue\n",
        "    for rdr in luke_wide.columns:\n",
        "        cell_text = luke_wide.at[res, rdr]\n",
        "        if not isinstance(cell_text, str) or not cell_text:\n",
        "            continue\n",
        "\n",
        "        hit = cell_text.endswith(\"*\")\n",
        "        if hit:\n",
        "            cell_text = cell_text[:-1]\n",
        "\n",
        "        m = re.match(r\"^\\s*([0-9]*\\.?[0-9]+)\\s*\\(n\\s*=\\s*(\\d+)\\)\\s*$\", cell_text)\n",
        "        if not m:\n",
        "            continue\n",
        "\n",
        "        delta_val = float(m.group(1))\n",
        "        n_val     = int(m.group(2))\n",
        "\n",
        "        base = matrix_parsed.at[res, rdr]\n",
        "\n",
        "        if k_luke is not None and (0, k_luke) not in base:\n",
        "            base.append((0, k_luke))                 # Luke presence tag\n",
        "\n",
        "        pair = (round(delta_val, 4), int(n_val))      # Luke numeric pair\n",
        "        if pair not in base:\n",
        "            base.append(pair)\n",
        "            appended += 1\n",
        "\n",
        "        matrix_parsed.at[res, rdr] = base\n",
        "\n",
        "print(f\"Luke integration: rows={len(luke_wide.index)}, cols={len(luke_wide.columns)}, appended pairs={appended}\")\n",
        "\n",
        "# ---------- 12) Indices ----------\n",
        "def _load_mod_index_from_sheet(xls_obj):\n",
        "    try:\n",
        "        df = pd.read_excel(xls_obj, \"Mod Indexes\", dtype={\"mod\": str, \"index\": float})\n",
        "        df = df.dropna(subset=[\"mod\", \"index\"]).copy()\n",
        "        df[\"mod\"] = df[\"mod\"].astype(str).str.strip().str.lower()\n",
        "        df[\"index\"] = df[\"index\"].astype(int)\n",
        "        if df[\"mod\"].duplicated(keep=False).any():\n",
        "            d = df[df[\"mod\"].duplicated(keep=False)].sort_values(\"mod\")\n",
        "            raise ValueError(\"Duplicate mod labels in 'Mod Indexes':\\n\" + d.to_string(index=False))\n",
        "        if df[\"index\"].duplicated(keep=False).any():\n",
        "            d = df[df[\"index\"].duplicated(keep=False)].sort_values(\"index\")\n",
        "            raise ValueError(\"Duplicate mod indices in 'Mod Indexes':\\n\" + d.to_string(index=False))\n",
        "        return dict(zip(df[\"mod\"], df[\"index\"])), df\n",
        "    except Exception as e:\n",
        "        print(f\"‚ÑπÔ∏è  Using built-in mod index map (could not load/validate sheet: {e})\")\n",
        "        return None, None\n",
        "\n",
        "MOD_INDEX, _mod_sheet_df_in = _load_mod_index_from_sheet(xls)\n",
        "if MOD_INDEX is None:\n",
        "    MOD_INDEX = {\"none\":0,\"ac\":1,\"me1\":2,\"me2\":3,\"me3\":4,\"ph\":5,\"lac\":6,\"ub\":7,\"ser\":8,\"cit\":9,\"prop\":10,\"cro\":11,\"brut\":12}\n",
        "MOD_SYNONYM = {\"me\":\"me1\", \"but\":\"brut\"}\n",
        "\n",
        "# =========================\n",
        "# NEW (ADDITIVE): Auto-extend MOD_INDEX if new mod tags appear in residue labels\n",
        "# =========================\n",
        "def _discover_mod_tags_from_labels(labels):\n",
        "    tags = set()\n",
        "    for lbl in labels:\n",
        "        if lbl is None or (isinstance(lbl, float) and np.isnan(lbl)):\n",
        "            continue\n",
        "        s = str(lbl).split(\"(\")[0].strip()\n",
        "        # remove whitespace\n",
        "        s = re.sub(r\"\\s+\", \"\", s)\n",
        "        # split by \"/\" to catch multi-marks\n",
        "        parts = s.split(\"/\")\n",
        "        for p in parts:\n",
        "            # match like H3K27ac, H2BK120ub, etc.\n",
        "            m = re.match(r\"^H\\d+(?:\\.\\d+)?[ABab]?[KSTRQYHDE]\\d+([A-Za-z0-9]+)?$\", p)\n",
        "            if not m:\n",
        "                continue\n",
        "            raw_tag = (m.group(1) or \"\").lower()\n",
        "            canon_tag = MOD_SYNONYM.get(raw_tag, raw_tag)\n",
        "            if canon_tag == \"\":\n",
        "                canon_tag = \"none\"\n",
        "            tags.add(canon_tag)\n",
        "    return tags\n",
        "\n",
        "def _extend_mod_index_inplace(mod_index_map, observed_tags):\n",
        "    missing = [t for t in sorted(observed_tags) if t not in mod_index_map]\n",
        "    if not missing:\n",
        "        return []\n",
        "    max_idx = max(int(v) for v in mod_index_map.values()) if mod_index_map else 0\n",
        "    added = []\n",
        "    for t in missing:\n",
        "        max_idx += 1\n",
        "        mod_index_map[t] = max_idx\n",
        "        added.append((t, max_idx))\n",
        "    return added\n",
        "\n",
        "# scan all residue labels we currently know about (matrix index + refs_df residues + Luke residues if present)\n",
        "_all_res_labels = set()\n",
        "_all_res_labels.update([x for x in matrix_parsed.index if _valid_residue_label(x)])\n",
        "_all_res_labels.update([x for x in refs_df[\"Histone residues\"].astype(str).tolist() if str(x).strip() != \"\"])\n",
        "if 'luke_sheet_df_wide' in locals() and luke_sheet_df_wide is not None:\n",
        "    _all_res_labels.update([x for x in luke_sheet_df_wide.index if _valid_residue_label(x)])\n",
        "\n",
        "_observed_tags = _discover_mod_tags_from_labels(_all_res_labels)\n",
        "_added_mods = _extend_mod_index_inplace(MOD_INDEX, _observed_tags)\n",
        "if _added_mods:\n",
        "    print(\"üß© Mod Indexes updated with new tags:\", \", \".join([f\"{m}‚Üí{i}\" for m,i in _added_mods]))\n",
        "\n",
        "def parse_mark_full(lbl):\n",
        "    s = str(lbl).split('(')[0].strip()\n",
        "    mH = re.match(r'H(\\d+(?:\\.\\d+)?)', s, flags=re.I)\n",
        "    if not mH:\n",
        "        return pd.Series({'histone':None,'site':[],'mod':[]})\n",
        "    hist = float(mH.group(1))\n",
        "    rest = s[mH.end():]\n",
        "    if rest.startswith(('A','B','a','b')):\n",
        "        rest = rest[1:]\n",
        "    rest = rest.lstrip('/')\n",
        "    sites, mods = [], []\n",
        "    for p in [p for p in rest.split('/') if p]:\n",
        "        m = re.match(r'([KSTRQYHDE])(\\d+)([A-Za-z0-9]+)?', p.strip(), flags=re.I)\n",
        "        if not m: continue\n",
        "        sites.append(int(m.group(2)))\n",
        "        raw_tag = (m.group(3) or \"\").lower()\n",
        "        canon_tag = MOD_SYNONYM.get(raw_tag, raw_tag)\n",
        "        if canon_tag == \"\":\n",
        "            canon_tag = \"none\"\n",
        "        mods.append(MOD_INDEX.get(canon_tag, 0))\n",
        "    return pd.Series({'histone':hist,'site':sites,'mod':mods})\n",
        "\n",
        "# ---------- 12b) DEDUPE numeric pairs by n (presence untouched) ----------\n",
        "def _is_presence_pair2(t):\n",
        "    return (isinstance(t, tuple) and len(t)==2 and t[0]==0\n",
        "            and isinstance(t[1], (int,float)) and float(t[1]).is_integer())\n",
        "\n",
        "def _dedupe_numeric_pairs(lst):\n",
        "    if not isinstance(lst, list): return lst\n",
        "    by_n = {}\n",
        "    others = []\n",
        "    for t in lst:\n",
        "        if isinstance(t, tuple) and len(t)==2 and all(isinstance(x,(int,float)) for x in t):\n",
        "            if _is_presence_pair2(t):\n",
        "                others.append(t)\n",
        "            else:\n",
        "                v, n = t\n",
        "                if n not in by_n or abs(v) > abs(by_n[n][0]):\n",
        "                    by_n[n] = (v, n)\n",
        "        else:\n",
        "            others.append(t)\n",
        "    return others + list(by_n.values())\n",
        "\n",
        "matrix_parsed = matrix_parsed.applymap(_dedupe_numeric_pairs)\n",
        "\n",
        "# ---------- 12c) Extractors + KD numeric‚Üístrength mapping ----------\n",
        "_STRENGTH_WORDS = {\n",
        "    \"no spot\":0,\"nospot\":0,\"none\":0,\"0\":0,\"0.0\":0,\n",
        "    \"very weak\":1,\"weak\":1,\"1\":1,\"1.0\":1,\n",
        "    \"moderate\":2,\"mod\":2,\"2\":2,\"2.0\":2,\n",
        "    \"strong\":3,\"3\":3,\"3.0\":3,\n",
        "    \"very strong\":4,\"vstrong\":4,\"4\":4,\"4.0\":4,\n",
        "}\n",
        "def _strength_from_any2(x):\n",
        "    if isinstance(x, (int, float)):\n",
        "        v = int(round(float(x)))\n",
        "        return v if 0 <= v <= 4 else None\n",
        "    s = str(x).strip().lower()\n",
        "    if s in _STRENGTH_WORDS: return _STRENGTH_WORDS[s]\n",
        "    s2 = s.replace(\",\", \" \")\n",
        "    m = re.search(r'\\b([0-4])(?:\\.0+)?\\b', s2)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def _choose_best_source_and_strength(items):\n",
        "    best = None\n",
        "    for t in items:\n",
        "        if isinstance(t, tuple) and len(t)>=3 and t[0]==0:\n",
        "            try: key = int(t[1])\n",
        "            except Exception: continue\n",
        "            strength = _strength_from_any2(t[2])\n",
        "            if strength is None: continue\n",
        "            if best is None or strength > best[1]:\n",
        "                best = (key, strength)\n",
        "    return best if best else (None, None)\n",
        "\n",
        "def _find_kd_string(items):\n",
        "    def _looks_like_kd_numeric(s):\n",
        "        s = str(s).strip()\n",
        "        if re.fullmatch(r'[0-4](?:\\.0+)?', s): return False\n",
        "        return bool(re.fullmatch(r'\\d*\\.?\\d+(?:\\s*(?:nM|uM|¬µM|mM))?', s))\n",
        "    for t in items:\n",
        "        if isinstance(t, tuple) and len(t)>=3 and t[0]==0 and isinstance(t[2], str):\n",
        "            s = t[2]\n",
        "            if ('¬±' in s) or ('+/-' in s) or _looks_like_kd_numeric(s):\n",
        "                try: k = int(t[1])\n",
        "                except Exception: k = None\n",
        "                return (k, s)\n",
        "    for t in items:\n",
        "        if isinstance(t, str):\n",
        "            s = t\n",
        "            if ('¬±' in s) or ('+/-' in s) or _looks_like_kd_numeric(s):\n",
        "                return (None, s)\n",
        "    return (None, 'NA')\n",
        "\n",
        "def _extract_kd_uM(kd_str: str):\n",
        "    if kd_str in (None, 'NA') or not isinstance(kd_str, str):\n",
        "        return np.nan\n",
        "    s = kd_str.strip()\n",
        "    s = re.sub(r'\\s*¬±\\s*[\\d\\.]+','', s)\n",
        "    s = re.sub(r'\\s*\\+/-\\s*[\\d\\.]+','', s)\n",
        "    s = re.sub(r'\\([^)]*\\)','', s)\n",
        "    s = s.replace(',', ' ')\n",
        "    m = re.search(r'([0-9]*\\.?[0-9]+)\\s*([mun¬µ]M|mM|uM|nM)?', s, flags=re.I)\n",
        "    if not m:\n",
        "        return np.nan\n",
        "    val = float(m.group(1))\n",
        "    unit = m.group(2)\n",
        "    if not unit:\n",
        "        unit = 'uM'\n",
        "    unit = unit.replace('¬µ','u')\n",
        "    u = unit.lower()\n",
        "    if u == 'nm': return val / 1000.0\n",
        "    if u == 'um': return val\n",
        "    if u == 'mm': return val * 1000.0\n",
        "    return np.nan\n",
        "\n",
        "def _kd_to_strength_uM(kd_uM: float):\n",
        "    if not np.isfinite(kd_uM): return None\n",
        "    if kd_uM <= 0.05:        return 4\n",
        "    elif kd_uM <= 0.5:       return 3\n",
        "    elif kd_uM <= 5:         return 2\n",
        "    elif kd_uM <= 50:        return 1\n",
        "    else:                    return 0\n",
        "\n",
        "def _find_luke(items):\n",
        "    for t in items:\n",
        "        if isinstance(t, tuple) and len(t)==2 and all(isinstance(x,(int,float)) for x in t):\n",
        "            if _is_presence_pair2(t):  # skip presence (0,key)\n",
        "                continue\n",
        "            v, n = t\n",
        "            return (f\"{v:.4f}\", int(n))\n",
        "    return ('NA', 'NA')\n",
        "\n",
        "def _presence_key(items, k_luke=None):\n",
        "    if k_luke is not None:\n",
        "        for t in items:\n",
        "            if _is_presence_pair2(t) and int(t[1]) == int(k_luke):\n",
        "                return int(k_luke)\n",
        "    for t in items:\n",
        "        if _is_presence_pair2(t):\n",
        "            return int(t[1])\n",
        "    return None\n",
        "\n",
        "# ---------- 12d) Build unified 5-field tuples ----------\n",
        "# (source_key, kd_value_str_or_'NA', kd_strength_index_or_'NA', luke_delta_or_'NA', luke_n_or_'NA')\n",
        "unified = pd.DataFrame(index=matrix_parsed.index, columns=matrix_parsed.columns)\n",
        "\n",
        "k_luke_from_keys = None\n",
        "try:\n",
        "    k_luke_from_keys = int(keys_df.loc[keys_df['source_name']==\"Luke's paper\",'key'].iloc[0])\n",
        "except Exception:\n",
        "    k_luke_from_keys = None\n",
        "\n",
        "for r_i in range(len(unified.index)):\n",
        "    row_vals = matrix_parsed.iloc[r_i]\n",
        "    for c_i in range(len(unified.columns)):\n",
        "        items = row_vals.iloc[c_i]\n",
        "        if not isinstance(items, list) or len(items)==0:\n",
        "            unified.iat[r_i, c_i] = \"\"\n",
        "            continue\n",
        "\n",
        "        best_src_key, best_strength = _choose_best_source_and_strength(items)\n",
        "        kd_src, kd_str              = _find_kd_string(items)\n",
        "        luke_delta, luke_n          = _find_luke(items)\n",
        "        present_key                 = _presence_key(items, k_luke_from_keys)\n",
        "\n",
        "        kd_strength_from_numeric = None\n",
        "        if kd_str != 'NA':\n",
        "            kd_uM = _extract_kd_uM(kd_str)\n",
        "            kd_strength_from_numeric = _kd_to_strength_uM(kd_uM)\n",
        "\n",
        "        src_final = kd_src if kd_src is not None else (best_src_key if best_src_key is not None else present_key)\n",
        "\n",
        "        strength_final = None\n",
        "        if kd_strength_from_numeric is not None:\n",
        "            strength_final = kd_strength_from_numeric\n",
        "        elif best_strength is not None:\n",
        "            strength_final = best_strength\n",
        "\n",
        "        if any([src_final is not None, kd_str!='NA', strength_final is not None, luke_delta!='NA', luke_n!='NA']):\n",
        "            unified.iat[r_i, c_i] = (\n",
        "                src_final if src_final is not None else 'NA',\n",
        "                kd_str,\n",
        "                strength_final if strength_final is not None else 'NA',\n",
        "                luke_delta, luke_n\n",
        "            )\n",
        "        else:\n",
        "            unified.iat[r_i, c_i] = \"\"\n",
        "\n",
        "# ---------- 12e) Kd_matrix_indexed with parsing columns ----------\n",
        "df_kd = unified.reset_index().rename(columns={'index':'Histone residues'})\n",
        "parsed_cols = df_kd['Histone residues'].apply(parse_mark_full)\n",
        "df_kd[['histone','site','mod']] = parsed_cols\n",
        "readers = [c for c in df_kd.columns if c not in {\"Histone residues\",\"histone\",\"site\",\"mod\"}]\n",
        "\n",
        "_DESC_COLS = {\"Histone residues\",\"histone\",\"site\",\"mod\"}\n",
        "_reader_cols = [c for c in df_kd.columns if c not in _DESC_COLS]\n",
        "\n",
        "def _cell_has_payload(x):\n",
        "    if isinstance(x, tuple) and len(x) == 5:\n",
        "        return any(v not in (\"\", \"NA\", None) for v in x)\n",
        "    if isinstance(x, str):\n",
        "        return x.strip() != \"\"\n",
        "    return pd.notna(x)\n",
        "\n",
        "label = df_kd[\"Histone residues\"].astype(str)\n",
        "is_blank = label.map(lambda s: s.strip() == \"\")\n",
        "is_nan_str = label.map(lambda s: s.strip().lower() == \"nan\")\n",
        "has_any = df_kd[_reader_cols].applymap(_cell_has_payload).any(axis=1)\n",
        "\n",
        "to_drop = (is_blank | is_nan_str) & (~has_any)\n",
        "if to_drop.any():\n",
        "    dropped = df_kd.loc[to_drop, \"Histone residues\"].astype(str).tolist()\n",
        "    print(f\"üßπ Removing {to_drop.sum()} unlabeled rows with no data (e.g., {', '.join(dropped[:5])})\")\n",
        "    df_kd = df_kd.loc[~to_drop].reset_index(drop=True)\n",
        "\n",
        "force_drop_nan_label = False\n",
        "if force_drop_nan_label:\n",
        "    is_nan_str_any = df_kd[\"Histone residues\"].astype(str).str.strip().str.lower().eq(\"nan\")\n",
        "    if is_nan_str_any.any():\n",
        "        print(f\"‚ö†Ô∏è Removing {is_nan_str_any.sum()} 'nan' residue rows by policy.\")\n",
        "        df_kd = df_kd.loc[~is_nan_str_any].reset_index(drop=True)\n",
        "\n",
        "_DENYLIST_TARGETED = {\n",
        "    \"dnamotif\",\"h1\",\"h1.4\",\"h2ax\",\"h2aac\",\"h2b\",\"h2bac\",\"h3\",\"h3andh4\",\n",
        "    \"h3r2\",\"h3ac\",\"h4\",\"h4kac\",\"h4ac\",\"hkme\",\"mcg\"\n",
        "}\n",
        "res_norm = df_kd[\"Histone residues\"].astype(str).map(lambda s: re.sub(r\"\\s+\",\"\", s).strip().lower())\n",
        "is_denylisted = res_norm.isin(_DENYLIST_TARGETED)\n",
        "has_any_data  = df_kd[_reader_cols].applymap(_cell_has_payload).any(axis=1)\n",
        "\n",
        "to_drop = is_denylisted & (~has_any_data)\n",
        "drop_labels = df_kd.loc[to_drop, \"Histone residues\"].astype(str).tolist()\n",
        "if drop_labels:\n",
        "    print(f\"üßπ Removing {len(drop_labels)} placeholder rows with no data: \"\n",
        "          + \", \".join(drop_labels[:8]) + (\"...\" if len(drop_labels) > 8 else \"\"))\n",
        "\n",
        "df_kd = df_kd.loc[~to_drop].reset_index(drop=True)\n",
        "\n",
        "# ---------- 12f) Build \"Kd_matrix_indexed_MY_ONLY\" (drop Luke tuples) ----------\n",
        "def _na_like(v):\n",
        "    if v is None:\n",
        "        return True\n",
        "    if isinstance(v, float):\n",
        "        try:\n",
        "            return np.isnan(v)\n",
        "        except Exception:\n",
        "            pass\n",
        "    s = str(v).strip()\n",
        "    return s == \"\" or s.upper() == \"NA\"\n",
        "\n",
        "def _is_luke_tuple_5(t):\n",
        "    return isinstance(t, tuple) and len(t) == 5 and (not _na_like(t[3]) or not _na_like(t[4]))\n",
        "\n",
        "df_kd_my_only = df_kd.copy()\n",
        "for c in readers:\n",
        "    df_kd_my_only[c] = df_kd_my_only[c].apply(lambda x: \"\" if _is_luke_tuple_5(x) else x)\n",
        "\n",
        "# ---------- 12g) Luke's paper plus: quartiles from position 4 (ONLY), restricted to Luke's interactions ----------\n",
        "# UPDATE: include ALL cells present in Luke's wide sheet (even blanks) as interactions.\n",
        "# Blank/missing/‚â§0 pos4 ‚Üí Bucket 0 (\"0 - No Spot\").\n",
        "\n",
        "def _to_float_or_nan(v):\n",
        "    if _na_like(v): return np.nan\n",
        "    try:\n",
        "        return float(str(v).strip())\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "allowed_pairs = set()\n",
        "if 'luke_sheet_df_wide' in locals() and luke_sheet_df_wide is not None:\n",
        "    # NEW: use full cartesian of Luke's rows √ó columns, regardless of value (counts blanks as interactions)\n",
        "    for res in luke_sheet_df_wide.index:\n",
        "        for rdr in luke_sheet_df_wide.columns:\n",
        "            allowed_pairs.add((res, rdr))\n",
        "else:\n",
        "    # fallback: only those in df_kd that actually have Luke tuples (kept as previous behavior)\n",
        "    for _, row in df_kd.iterrows():\n",
        "        residue = row[\"Histone residues\"]\n",
        "        for rdr in readers:\n",
        "            cell = row[rdr]\n",
        "            if isinstance(cell, tuple) and len(cell) == 5 and not _na_like(cell[3]):\n",
        "                allowed_pairs.add((residue, rdr))\n",
        "\n",
        "records_pos = []\n",
        "records_zero = []\n",
        "\n",
        "for _, row in df_kd.iterrows():\n",
        "    residue = row[\"Histone residues\"]\n",
        "    for rdr in readers:\n",
        "        if (residue, rdr) not in allowed_pairs:\n",
        "            continue  # only things present on Luke's sheet\n",
        "        cell = row[rdr]\n",
        "        if isinstance(cell, tuple) and len(cell) == 5:\n",
        "            _, _, pos3_strength, luke_delta, _ = cell\n",
        "            d = _to_float_or_nan(luke_delta)\n",
        "            if not np.isfinite(d) or d <= 0.0:\n",
        "                records_zero.append({\n",
        "                    \"Bucket\": 0,\n",
        "                    \"Strength label\": \"0 - No Spot\",\n",
        "                    \"Histone residues\": residue,\n",
        "                    \"Reader\": rdr,\n",
        "                    \"pos3 strength (existing)\": pos3_strength if pos3_strength != \"NA\" else np.nan,\n",
        "                    \"pos4 (Luke delta)\": np.nan if not np.isfinite(d) else float(d)\n",
        "                })\n",
        "            else:\n",
        "                records_pos.append({\n",
        "                    \"Histone residues\": residue,\n",
        "                    \"Reader\": rdr,\n",
        "                    \"pos3 strength (existing)\": pos3_strength if pos3_strength != \"NA\" else np.nan,\n",
        "                    \"pos4 (Luke delta)\": float(d)\n",
        "                })\n",
        "        else:\n",
        "            # Present in Luke's table but no tuple/value recorded ‚Üí treat as No Spot\n",
        "            records_zero.append({\n",
        "                \"Bucket\": 0,\n",
        "                \"Strength label\": \"0 - No Spot\",\n",
        "                \"Histone residues\": residue,\n",
        "                \"Reader\": rdr,\n",
        "                \"pos3 strength (existing)\": np.nan,\n",
        "                \"pos4 (Luke delta)\": np.nan\n",
        "            })\n",
        "\n",
        "if len(records_pos) == 0 and len(records_zero) == 0:\n",
        "    lukes_plus_df_out = pd.DataFrame(columns=[\n",
        "        \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "    ])\n",
        "else:\n",
        "    df_pos = pd.DataFrame.from_records(records_pos)\n",
        "    if not df_pos.empty:\n",
        "        bins = pd.qcut(df_pos[\"pos4 (Luke delta)\"], 4, labels=[1,2,3,4], duplicates=\"drop\")\n",
        "        df_pos[\"Bucket\"] = bins.astype(float).astype(\"Int64\")\n",
        "        label_map = {1:\"1 - Very weak\", 2:\"2 - Moderate\", 3:\"3 - Strong\", 4:\"4 - Very Strong\"}\n",
        "        df_pos[\"Strength label\"] = df_pos[\"Bucket\"].map(label_map)\n",
        "        df_pos = df_pos[[\n",
        "            \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "        ]].sort_values([\"Bucket\",\"pos4 (Luke delta)\"], ascending=[True,True])\n",
        "    else:\n",
        "        df_pos = pd.DataFrame(columns=[\n",
        "            \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "        ])\n",
        "\n",
        "    df_zero = pd.DataFrame.from_records(records_zero, columns=[\n",
        "        \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "    ])\n",
        "\n",
        "    lukes_plus_df_out = pd.concat([df_zero, df_pos], ignore_index=True)\n",
        "\n",
        "# =========================\n",
        "# NEW (ADDITIVE) 12h) Luke's paper plus HITS:\n",
        "#   - delta < 0.15  -> Bucket 0 (\"0 - No Spot\")\n",
        "#   - delta >= 0.15 -> quartiles into 1..4, based ONLY on those >= 0.15\n",
        "#   - still restricted to Luke‚Äôs interaction space (allowed_pairs)\n",
        "# =========================\n",
        "HIT_THRESHOLD = 0.15\n",
        "\n",
        "records_hit_pos = []\n",
        "records_hit_zero = []\n",
        "\n",
        "for _, row in df_kd.iterrows():\n",
        "    residue = row[\"Histone residues\"]\n",
        "    for rdr in readers:\n",
        "        if (residue, rdr) not in allowed_pairs:\n",
        "            continue\n",
        "        cell = row[rdr]\n",
        "        if isinstance(cell, tuple) and len(cell) == 5:\n",
        "            _, _, pos3_strength, luke_delta, _ = cell\n",
        "            d = _to_float_or_nan(luke_delta)\n",
        "\n",
        "            # Anything blank/NA or below threshold => 0\n",
        "            if (not np.isfinite(d)) or (d < HIT_THRESHOLD):\n",
        "                records_hit_zero.append({\n",
        "                    \"Bucket\": 0,\n",
        "                    \"Strength label\": \"0 - No Spot\",\n",
        "                    \"Histone residues\": residue,\n",
        "                    \"Reader\": rdr,\n",
        "                    \"pos3 strength (existing)\": pos3_strength if pos3_strength != \"NA\" else np.nan,\n",
        "                    \"pos4 (Luke delta)\": np.nan if not np.isfinite(d) else float(d)\n",
        "                })\n",
        "            else:\n",
        "                records_hit_pos.append({\n",
        "                    \"Histone residues\": residue,\n",
        "                    \"Reader\": rdr,\n",
        "                    \"pos3 strength (existing)\": pos3_strength if pos3_strength != \"NA\" else np.nan,\n",
        "                    \"pos4 (Luke delta)\": float(d)\n",
        "                })\n",
        "        else:\n",
        "            # Present in Luke's table but no tuple/value recorded -> treat as below threshold => 0\n",
        "            records_hit_zero.append({\n",
        "                \"Bucket\": 0,\n",
        "                \"Strength label\": \"0 - No Spot\",\n",
        "                \"Histone residues\": residue,\n",
        "                \"Reader\": rdr,\n",
        "                \"pos3 strength (existing)\": np.nan,\n",
        "                \"pos4 (Luke delta)\": np.nan\n",
        "            })\n",
        "\n",
        "if len(records_hit_pos) == 0 and len(records_hit_zero) == 0:\n",
        "    lukes_plus_hits_df_out = pd.DataFrame(columns=[\n",
        "        \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "    ])\n",
        "else:\n",
        "    df_hit_pos = pd.DataFrame.from_records(records_hit_pos)\n",
        "    if not df_hit_pos.empty:\n",
        "        bins = pd.qcut(df_hit_pos[\"pos4 (Luke delta)\"], 4, labels=[1,2,3,4], duplicates=\"drop\")\n",
        "        df_hit_pos[\"Bucket\"] = bins.astype(float).astype(\"Int64\")\n",
        "        label_map = {1:\"1 - Very weak\", 2:\"2 - Moderate\", 3:\"3 - Strong\", 4:\"4 - Very Strong\"}\n",
        "        df_hit_pos[\"Strength label\"] = df_hit_pos[\"Bucket\"].map(label_map)\n",
        "        df_hit_pos = df_hit_pos[[\n",
        "            \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "        ]].sort_values([\"Bucket\",\"pos4 (Luke delta)\"], ascending=[True,True])\n",
        "    else:\n",
        "        df_hit_pos = pd.DataFrame(columns=[\n",
        "            \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "        ])\n",
        "\n",
        "    df_hit_zero = pd.DataFrame.from_records(records_hit_zero, columns=[\n",
        "        \"Bucket\",\"Strength label\",\"Histone residues\",\"Reader\",\"pos3 strength (existing)\",\"pos4 (Luke delta)\"\n",
        "    ])\n",
        "\n",
        "    lukes_plus_hits_df_out = pd.concat([df_hit_zero, df_hit_pos], ignore_index=True)\n",
        "\n",
        "# =========================\n",
        "# NEW (ADDITIVE) 12i) Propagate Luke HIT-buckets into Kd_matrix_indexed (pos3) for the same interactions\n",
        "# Rule: Only fill pos3 if it's currently NA/blank (do NOT overwrite existing pos3 strength).\n",
        "# =========================\n",
        "_bucket_map_hits = {}\n",
        "try:\n",
        "    for _, rr in lukes_plus_hits_df_out.iterrows():\n",
        "        _bucket_map_hits[(str(rr[\"Histone residues\"]), str(rr[\"Reader\"]))] = int(rr[\"Bucket\"]) if pd.notna(rr[\"Bucket\"]) else 0\n",
        "except Exception:\n",
        "    _bucket_map_hits = {}\n",
        "\n",
        "def _fill_pos3_from_luke_hits(cell, residue, reader):\n",
        "    if not (isinstance(cell, tuple) and len(cell) == 5):\n",
        "        return cell\n",
        "    src, kd_str, pos3, luke_delta, luke_n = cell\n",
        "    # only apply when interaction exists in Luke space\n",
        "    if (str(residue), str(reader)) not in _bucket_map_hits:\n",
        "        return cell\n",
        "    # only fill if pos3 is NA/blank\n",
        "    if _na_like(pos3):\n",
        "        b = _bucket_map_hits[(str(residue), str(reader))]\n",
        "        return (src, kd_str, b, luke_delta, luke_n)\n",
        "    return cell\n",
        "\n",
        "# apply fill across df_kd (but only reader columns)\n",
        "_res_list = df_kd[\"Histone residues\"].astype(str).tolist()\n",
        "for i, residue in enumerate(_res_list):\n",
        "    for rdr in readers:\n",
        "        df_kd.at[i, rdr] = _fill_pos3_from_luke_hits(df_kd.at[i, rdr], residue, rdr)\n",
        "\n",
        "# ---------- 13) Index tabs ----------\n",
        "# Mod Indexes: now rebuild from (possibly-extended) MOD_INDEX to ensure it updates when new mods appear\n",
        "mod_index_df = pd.DataFrame(\n",
        "    [{\"mod\":k, \"index\":v} for k, v in sorted(MOD_INDEX.items(), key=lambda kv: kv[1])],\n",
        "    columns=[\"mod\",\"index\"]\n",
        ")\n",
        "\n",
        "hist_series = pd.to_numeric(df_kd['histone'], errors='coerce')\n",
        "hist_vals = sorted([v for v in hist_series.dropna().unique()])\n",
        "if 2.0 in hist_vals:\n",
        "    hist_index_df = pd.DataFrame(\n",
        "        [{'histone':'H2A','index':2},{'histone':'H2B','index':2}] +\n",
        "        [{'histone':int(h) if float(h).is_integer() else h,'index':h} for h in hist_vals if h!=2.0]\n",
        "    )\n",
        "else:\n",
        "    hist_index_df = pd.DataFrame([{'histone':h,'index':h} for h in hist_vals])\n",
        "\n",
        "site_vals = set()\n",
        "for sub in df_kd['site']:\n",
        "    if isinstance(sub, list):\n",
        "        site_vals.update(sub)\n",
        "site_index_df = pd.DataFrame([{'site':s,'index':s} for s in sorted(site_vals)])\n",
        "\n",
        "ik = []\n",
        "for i, r in enumerate(readers, start=1):\n",
        "    ik.append({'type':'reader','label':r,'index':i})\n",
        "for i, res in enumerate(df_kd['Histone residues'], start=1):\n",
        "    ik.append({'type':'residue','label':res,'index':i})\n",
        "index_keys_df = pd.DataFrame(ik, columns=['type','label','index'])\n",
        "\n",
        "# ---------- 13b) Source 11 sheet (legacy 11 only) ----------\n",
        "def _only_source_key_triples(lst, key):\n",
        "    if not isinstance(lst, list): return []\n",
        "    return [t for t in lst if isinstance(t, tuple) and len(t)>=3 and t[1]==key]\n",
        "s11_only = matrix_parsed.applymap(lambda lst: _only_source_key_triples(lst, 11))\n",
        "s11_df_wide = s11_only.map(list_to_tuple_cell).reset_index().rename(columns={'index':'Histone residues'})\n",
        "s11_parsed = s11_df_wide['Histone residues'].apply(parse_mark_full)\n",
        "s11_df_wide[['histone','site','mod']] = s11_parsed\n",
        "s11_readers = [c for c in s11_df_wide.columns if c not in {\"Histone residues\",\"histone\",\"site\",\"mod\"}]\n",
        "s11_df_out = s11_df_wide[['histone','site','mod','Histone residues'] + s11_readers]\n",
        "s11_title_safe = \"Source 11\"\n",
        "\n",
        "# =========================\n",
        "# NEW (ADDITIVE) 13c) Re-add Sequences sheet (preserve existing sequences if present)\n",
        "# =========================\n",
        "_GENERAL_HISTONES = [\"H1.4\", \"H2\", \"H1B\", \"H2A\", \"H2B\", \"H3\", \"H4\"]\n",
        "\n",
        "def _normalize_sequences_sheet(df):\n",
        "    # accept flexible headers; normalize to Type/Name/Sequence\n",
        "    col_map = {}\n",
        "    for c in df.columns:\n",
        "        lc = str(c).strip().lower()\n",
        "        if lc in {\"type\", \"category\"}:\n",
        "            col_map[c] = \"Type\"\n",
        "        elif lc in {\"name\", \"protein\", \"histone\", \"reader\"}:\n",
        "            col_map[c] = \"Name\"\n",
        "        elif \"seq\" in lc:\n",
        "            col_map[c] = \"Sequence\"\n",
        "    df2 = df.rename(columns=col_map).copy()\n",
        "    for need in [\"Type\", \"Name\", \"Sequence\"]:\n",
        "        if need not in df2.columns:\n",
        "            df2[need] = \"\"\n",
        "    df2[\"Type\"] = df2[\"Type\"].astype(str).str.strip()\n",
        "    df2[\"Name\"] = df2[\"Name\"].astype(str).str.strip()\n",
        "    df2[\"Sequence\"] = df2[\"Sequence\"].astype(str)\n",
        "    df2 = df2[[\"Type\", \"Name\", \"Sequence\"]]\n",
        "    # drop fully blank names\n",
        "    df2 = df2[df2[\"Name\"].astype(str).str.strip().ne(\"\")]\n",
        "    return df2\n",
        "\n",
        "_existing_sequences_df = None\n",
        "if \"Sequences\" in xls.sheet_names:\n",
        "    try:\n",
        "        _existing_sequences_df = pd.read_excel(xls, \"Sequences\", dtype=str)\n",
        "        _existing_sequences_df = _normalize_sequences_sheet(_existing_sequences_df)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not read existing 'Sequences' sheet; will recreate it. Reason: {e}\")\n",
        "        _existing_sequences_df = None\n",
        "\n",
        "# build base entries\n",
        "_seq_rows = []\n",
        "for h in _GENERAL_HISTONES:\n",
        "    _seq_rows.append({\"Type\": \"Histone\", \"Name\": h, \"Sequence\": \"\"})\n",
        "for r in readers:\n",
        "    _seq_rows.append({\"Type\": \"Reader\", \"Name\": str(r).strip(), \"Sequence\": \"\"})\n",
        "\n",
        "_sequences_df = pd.DataFrame(_seq_rows)\n",
        "\n",
        "# merge-preserve: keep any existing sequences, only add missing rows\n",
        "if _existing_sequences_df is not None and len(_existing_sequences_df) > 0:\n",
        "    # outer merge on Type+Name, prefer existing Sequence when non-empty\n",
        "    merged = _sequences_df.merge(\n",
        "        _existing_sequences_df,\n",
        "        on=[\"Type\", \"Name\"],\n",
        "        how=\"left\",\n",
        "        suffixes=(\"\", \"_old\")\n",
        "    )\n",
        "    def _pick_seq(row):\n",
        "        old = row.get(\"Sequence_old\", \"\")\n",
        "        new = row.get(\"Sequence\", \"\")\n",
        "        old_s = \"\" if pd.isna(old) else str(old)\n",
        "        new_s = \"\" if pd.isna(new) else str(new)\n",
        "        return old_s if old_s.strip() != \"\" else new_s\n",
        "    merged[\"Sequence\"] = merged.apply(_pick_seq, axis=1)\n",
        "    _sequences_df = merged[[\"Type\", \"Name\", \"Sequence\"]]\n",
        "\n",
        "# sort nicely\n",
        "_type_order = {\"Histone\": 0, \"Reader\": 1}\n",
        "_sequences_df[\"_t\"] = _sequences_df[\"Type\"].map(lambda x: _type_order.get(str(x), 99))\n",
        "_sequences_df = _sequences_df.sort_values([\"_t\", \"Name\"]).drop(columns=[\"_t\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- 14) Write back ----------\n",
        "with pd.ExcelWriter(kd_file, engine='openpyxl') as w:\n",
        "    df_kd.set_index(\"Histone residues\").to_excel(w, \"Kd_matrix_indexed\")\n",
        "    df_kd_my_only.set_index(\"Histone residues\").to_excel(w, \"Kd_matrix_indexed_MY_ONLY\")\n",
        "    HEATMAP_REF = \"heatmap_matrix_output_cleaned_corrected.xlsx\"\n",
        "    mask_heatmap = refs_df[\"Reference\"].astype(str).str.strip().eq(HEATMAP_REF)\n",
        "    refs_df.loc[mask_heatmap, \"kd_idx_auto\"] = np.nan\n",
        "    refs_df.to_excel(w, \"References\", index=False)\n",
        "    keys_df.to_excel(w, \"Keys\", index=False)\n",
        "    index_keys_df.to_excel(w, \"Index Keys\", index=False)\n",
        "    mod_index_df.to_excel(w, \"Mod Indexes\", index=False)\n",
        "    hist_index_df.to_excel(w, \"Histone number indexes\", index=False)\n",
        "    site_index_df.to_excel(w, \"Histone site indexes\", index=False)\n",
        "    pd.DataFrame({\n",
        "        \"Kd Indicator\":[\"No Spot\",\"Very weak\",\"Moderate\",\"Strong\",\"Very Strong\"],\n",
        "        \"Index\":[0,1,2,3,4]\n",
        "    }).to_excel(w, \"Kd Index Keys\", index=False)\n",
        "    s11_df_out.set_index(\"Histone residues\").to_excel(w, \"Source 11\")\n",
        "    if 'luke_sheet_df_wide' in locals() and luke_sheet_df_wide is not None:\n",
        "        luke_core = (luke_sheet_df_wide.reset_index()\n",
        "                     .rename(columns={'index':'Histone residues', 'Residue':'Histone residues'}))\n",
        "        luke_core[['histone','site','mod']] = luke_core['Histone residues'].apply(parse_mark_full)\n",
        "        (luke_core[['histone','site','mod','Histone residues'] + list(luke_sheet_df_wide.columns)]\n",
        "         .set_index(\"Histone residues\")\n",
        "         .to_excel(w, \"Luke's paper\"))\n",
        "    if 'lukes_plus_df_out' in locals() and lukes_plus_df_out is not None:\n",
        "        lukes_plus_df_out.to_excel(w, \"Luke's paper plus\", index=False)\n",
        "\n",
        "    # NEW (ADDITIVE): write the hit-thresholded version right after\n",
        "    if 'lukes_plus_hits_df_out' in locals() and lukes_plus_hits_df_out is not None:\n",
        "        lukes_plus_hits_df_out.to_excel(w, \"Luke's paper plus hits\", index=False)\n",
        "\n",
        "    # NEW (ADDITIVE): write Sequences sheet back\n",
        "    _sequences_df.to_excel(w, \"Sequences\", index=False)\n",
        "\n",
        "# --- Auto-download the updated workbook (Colab only) ---\n",
        "try:\n",
        "    from google.colab import files as _colab_files\n",
        "    _colab_files.download(str(kd_file))\n",
        "    print(f\"üì• Auto-download started for: {kd_file.name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Auto-download failed: {e}\")\n",
        "\n",
        "# ---------- 15) Interactive lookup ----------\n",
        "def _build_lookup_maps(index_keys_df, df_kd, readers):\n",
        "    rdr_map = {int(row['index']): row['label'] for _, row in index_keys_df[index_keys_df['type']=='reader'].iterrows()}\n",
        "    res_map = {int(row['index']): row['label'] for _, row in index_keys_df[index_keys_df['type']=='residue'].iterrows()}\n",
        "    rdr_norm = {re.sub(r\"\\s+\",\"\",c).lower(): c for c in readers}\n",
        "    res_norm = {re.sub(r\"\\s+\",\"\",r).lower(): r for r in df_kd[\"Histone residues\"].tolist()}\n",
        "    return rdr_map, res_map, rdr_norm, res_norm\n",
        "\n",
        "def _pick_one(options, header=\"Multiple matches; choose one\"):\n",
        "    for i, s in enumerate(options, 1):\n",
        "        print(f\"   {i}) {s}\")\n",
        "    sel = input(f\"{header} (1-{len(options)}, blank to cancel): \").strip()\n",
        "    if sel.isdigit() and 1 <= int(sel) <= len(options):\n",
        "        return options[int(sel)-1]\n",
        "    return None\n",
        "\n",
        "def _fuzzy_match_name(name, pool):\n",
        "    q = re.sub(r\"\\s+\",\"\", str(name)).lower()\n",
        "    if q in pool: return pool[q]\n",
        "    hits = [pool[k] for k in pool.keys() if q in k]\n",
        "    return hits\n",
        "\n",
        "def _lookup_ui(df_kd, readers, index_keys_df):\n",
        "    rdr_map, res_map, rdr_norm, res_norm = _build_lookup_maps(index_keys_df, df_kd, readers)\n",
        "    def _get_tuple(res_label, rdr_label):\n",
        "        if res_label not in df_kd[\"Histone residues\"].values:\n",
        "            return None, f\"Residue '{res_label}' not found.\"\n",
        "        if rdr_label not in readers:\n",
        "            return None, f\"Reader '{rdr_label}' not found.\"\n",
        "        row = df_kd.loc[df_kd[\"Histone residues\"]==res_label].iloc[0]\n",
        "        return row[rdr_label], None\n",
        "    print(\"\\nüîé Interactive lookup:\")\n",
        "    print(\"   (n) name pair   (i) index pair   (s) search names   (q) quit\")\n",
        "    while True:\n",
        "        mode = input(\"Lookup mode [n/i/s/q]: \").strip().lower()\n",
        "        if mode in {\"q\",\"\"}: break\n",
        "        if mode == \"s\":\n",
        "            q = input(\" Search term (reader or residue): \").strip()\n",
        "            r_hits = [v for k,v in rdr_norm.items() if q.lower() in k]\n",
        "            e_hits = [v for k,v in res_norm.items() if q.lower() in k]\n",
        "            print(f\" Readers ({len(r_hits)}): \" + (\", \".join(r_hits[:12]) + (\"...\" if len(r_hits)>12 else \"\")))\n",
        "            print(f\" Residues ({len(e_hits)}): \" + (\", \".join(e_hits[:12]) + (\"...\" if len(e_hits)>12 else \"\")))\n",
        "            continue\n",
        "        if mode == \"i\":\n",
        "            try:\n",
        "                ri = int(input(\" Reader index (from 'Index Keys'): \").strip())\n",
        "                ei = int(input(\" Residue index (from 'Index Keys'): \").strip())\n",
        "            except:\n",
        "                print(\"  ‚ùå Please enter integers.\"); continue\n",
        "            if ri not in rdr_map or ei not in res_map:\n",
        "                print(\"  ‚ùå Index not found in 'Index Keys'.\"); continue\n",
        "            rdr_label = rdr_map[ri]; res_label = res_map[ei]\n",
        "            val, err = _get_tuple(res_label, rdr_label)\n",
        "            if err: print(\"  ‚ùå\", err)\n",
        "            else:   print(f\"  ‚Üí {rdr_label} √ó {res_label} = {val if val!='' else '(empty)'}\")\n",
        "            continue\n",
        "        if mode == \"n\":\n",
        "            r_in = input(\" Reader name (full or part): \").strip()\n",
        "            e_in = input(\" Residue name (full or part): \").strip()\n",
        "            r_match = _fuzzy_match_name(r_in, rdr_norm)\n",
        "            rdr_label = _pick_one(r_match, \"Pick reader\") if isinstance(r_match, list) else r_match\n",
        "            if not rdr_label: print(\"  ‚ùå No reader match.\"); continue\n",
        "            e_match = _fuzzy_match_name(e_in, res_norm)\n",
        "            res_label = _pick_one(e_match, \"Pick residue\") if isinstance(e_match, list) else e_match\n",
        "            if not res_label: print(\"  ‚ùå No residue match.\"); continue\n",
        "            val, err = _get_tuple(res_label, rdr_label)\n",
        "            if err: print(\"  ‚ùå\", err)\n",
        "            else:   print(f\"  ‚Üí {rdr_label} √ó {res_label} = {val if val!='' else '(empty)'}\")\n",
        "\n",
        "try:\n",
        "    _lookup_ui(df_kd, readers, index_keys_df)\n",
        "except Exception as e:\n",
        "    print(\"Interactive lookup skipped due to error:\", e)\n",
        "\n",
        "print(\"\\n‚úÖ Done. Sheets written:\")\n",
        "print(\" - 'Luke's paper plus' (quartiles based on pos4 > 0.0; blanks/‚â§0 => 0)\")\n",
        "print(f\" - 'Luke's paper plus hits' (thresholded: pos4 < {HIT_THRESHOLD} => 0; pos4 ‚â• {HIT_THRESHOLD} quartiled into 1‚Äì4)\")\n",
        "print(\" - 'Kd_matrix_indexed' now fills pos3 with Luke hit-buckets ONLY when pos3 was NA/blank (no overwrites).\")\n",
        "print(\" - 'Mod Indexes', 'Histone number indexes', 'Histone site indexes' are rebuilt to reflect newly observed data.\")\n",
        "print(\" - 'Sequences' restored (preserves existing sequences if present; adds missing histones/readers).\")\n"
      ]
    }
  ]
}